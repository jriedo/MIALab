\documentclass[journal]{IEEEtran}

\usepackage[pdftex]{graphicx}
\graphicspath{{img/}}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{booktabs,siunitx}
\usepackage{threeparttable}
%\usepackage{subcaption}
\usepackage{multirow}
\usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}

% TODO: remove
	\usepackage{xcolor}
	\newcommand\TODO[1]{\textcolor{red}{TODO: #1}}
	\newcommand\FIXME[1]{\textcolor{blue}{FIXME: #1}}

\begin{document}
\title{Title of the Paper}


\author{Michael~Mueller,
        Jan~Riedo,
        Michael~Rebsamen% <-this % stops a space
\thanks{Biomedical Engineering, University of Bern}% <-this % stops a space
\thanks{Authors e-Mail: michael.mueller@students.unibe.ch, jan.riedo@students.unibe.ch, michael.rebsamen@students.unibe.ch}}% <-this % stops a space
\markboth{Biomedical Engineering, Medical Image Analysis Lab, \today}%
{Title of the Paper}
\maketitle

\begin{abstract}
Bla
\end{abstract}
\begin{IEEEkeywords}
MRI, Segmentation, Machine Learning, DF, kNN, SVM
\end{IEEEkeywords}


\section{Introduction}
Segmentation of brain tissues from magnetic resonance images (MRI) has many clinical applications. Clinicians gain useful information from a separation of tissue into its three main anatomical types: white matter, grey matter, and ventricles. However, manual segmentation of MRI is a labour-intensive task requiring expert skills. Fully automatic approaches for brain tissue segmentation are therefore a topic of active research. A good algorithm classifies the tissue types with high accuracy across a variety of images from different patients. Such a classification is a typical task for machine learning. These algorithms tend to perform well given enough training data during the learning phase. The availability of ground-truth data in sufficient quantity and quality for supervised learning is a particular challenge when working with medical images due to privacy concerns and the costs for manual segmentation. Optimization of the learning phase with a limited number of training data is therefore required.

\FIXME{kNN is a popular classification method for MR data and has successfully been applied in MR brain segmentation\cite{Anbeek2004,Cocosco2003,Warfield2000}}

\FIXME{Base paper on df \cite{Breiman2001}.}


\section{Methods}

\subsection{Dataset}
All experiments were conducted on a subset of 100 unrelated subjects from a dataset provided by the \textit{Human Connectome Project} \cite{van2013wu}. From each individual, a total of eight 3-tesla head MRI are available: T1 and T2-weighted image volumes not skull-stripped (but defaced for anonymization) and skull-stripped with a bias field correction, and both modalities once in native T1 space and once in MNI-atlas space \cite{mazziotta2001probabilistic}.

Ground-truth labels are automatically generated using \textit{FreeSurf}, assigning each voxel either to background, white matter, grey matter, or ventricles. The dataset was split in a training set with 70 images and a test set with 30 images.
\subsection{Pipeline}
\TODO{Describe whole pipeline (registration, pre-processing, feature extraction, ML classification, post-processing, evaluation}
\FIXME{During feature extraction, a random mask is applied in order to randomly select a fraction of the voxels available. The mask is adjustable individually for background, white matter, grey matter, and ventricles. It is crucial to optimize those parameters in order to obtain the best result.}

\subsection{Training}
\TODO{Describe training of machine learning algorithms}
\TODO{SVM gridsearch for hyperparameter tuning?}

\subsection{Performance Evaluation}
\TODO{Describe metric (dice score)}

\subsection{Infrastructure}
\TODO{Describe UBELIX, libraries}


\section{Results}
One major task to handle was the low value for the ventricles. Dice values above 0.5 were hard to achieve. One way to improve the dice for ventricles was to optimize the random mask with respect to the fraction of ventricle voxels taken into account. The effects of the random mask on the ventricle dice can be seen in Fig.~(\ref{f.random_mask}). Best results were achieved with a fraction of 0.004 ventricles, approximately the same fraction as for white matter and grey matter. All following results are based on this optimized mask.
\begin{figure}[h!]
	\centering
	\subfloat[]{\includegraphics[width=0.3\linewidth]{images/ven_0_4}}
	\hfill
	\subfloat[]{\includegraphics[width=0.3\linewidth]{images/ven_0_04}}
	\hfill
	\subfloat[]{\includegraphics[width=0.3\linewidth]{images/ven_0_004}}
	\caption{Optimization of the random mask parameter for ventricles. Fraction of ventricle voxels taken into account $f_v$ and dice value for this certain parameter $d_v$ are: ($f_v$ / $d_v$) (a) 0.4 / 0.22, (b) 0.04 / 0.44, and (c) 0.004 / 0.62.}
	\label{f.random_mask}
\end{figure}

\TODO{JR: subplot with 3 result images (good dice bad result, good dice good result, bad ground truth}



\TODO{JR: DF hyperparameter optimization, 3DPlot}
The decision forest algorithm was enhanced with normalized features, a higher number of ventricle voxels in the training set and the optimization of the hyperparameters (see Fig.~\ref{f.df_white}). With this settings, the max dice coefficient was lifted from 0.703 to 0.754. This result was achieved with 80 trees and 3000 max nodes.

\begin{figure}[h!]\label{f.df_white}
	\centering
	\includegraphics[width=0.48\textwidth]{images/df_grid}
	\caption{DF plot of grid search for white matter, grey matter and ventricles. The red cross marks the chosen hyperparameters number of trees~=~160 and maximum nodes per tree~=~3000. Color does not represent dice, the data is stretched individually for all three plots.}
\end{figure}

\TODO{JR: kNN optimization}

\begin{figure}\label{scatterplot}
dth=0.48\textwidth]{images/ScatterPlotMatrix}
nt}
g
aphics[width=0.48\textwidth]{images/DF_FeatEval_WSF_PP}
ngle feature and with preprocessing}
extwidth]{images/DF_FeatEval_WSF_NPP}
gle feature and without preprocessing}
dth]{images/DF_FeatEval_SF_PP}
le feature and with preprocessing }
textwidth]{images/DF_FeatEval_SF_NPP}
e feature and without preprocessing }
DF and SVM achieve a similar mean dice score but SVM has a lower variance for the ventricles.
\begin{figure}\label{f.boxplot}
	\centering
	\includegraphics[width=0.48\textwidth]{images/boxplot}
	\caption{Distribution of dice coefficients with optimal hyper-parameters for each algorithm on the full training set of 70 images.}
\end{figure}


\begin{table*}[t]
\renewcommand{\arraystretch}{1.2}
\newcommand\mulrow[2]{\multirow{#1}{*}{\shortstack[c]{#2}}}
\caption{Performance Comparison of ML Algorithms}
\label{tab:perf_compare}
\centering
\begin{threeparttable}
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}c*{6}{S[table-number-alignment=center,table-figures-decimal=2,table-auto-round]}@{}}
\toprule
Features & {Size Dataset} & {\shortstack[c]{DF}} & {\shortstack[c]{GMM}} & {\shortstack[c]{kNN}} & {\shortstack[c]{SGD}} & {\shortstack[c]{SVM}}\\
\midrule
\mulrow{3}{All\\(f1-f7)}
	& 3		&	{-}		& {-}	& {0.70/0.57/0.48}	& {-}	& {-}\\
	& 12		&	{0.84/0.80/0.52}		& {0.00/0.78/0.00}	& {0.75/0.66/0.67}	& {-}	& {0.83/0.81/0.58}\\
	& 70		&	{0.85/0.80/0.60}		& {-}	& {0.79/0.76/0.73}	& {0.82/0.80/0.33}	& {0.84/0.82/0.61}\\
\midrule
\mulrow{3}{Coordinates only\\(f1-f3)}
	& 3		&	{-}		& {-}	& {0.70/0.55/0.41}	& {-}	& {-}\\
	& 12		&	{-}		& {-}	& {0.74/0.63/0.56}	& {-}	& {-}\\
	& 70		&	{-}		& {-}	& {0.77/0.71/0.62}	& {-}	& {-}\\
\midrule
\mulrow{3}{All non-coordinates \\(f4-f7)}
	& 3		&	{-}		& {-}	& {0.85/0.80/0.45}	& {-}	& {-}\\
	& 12		&	{-}		& {-}	& {0.85/0.81/0.45}	& {-}	& {-}\\
	& 70		&	{-}		& {-}	& {0.85/0.81/0.54}	& {-}	& {-}\\
\bottomrule
\end{tabular*}
\begin{tablenotes}
\item Overview of achieved accuracy for the different algorithms. Mean dice scores for white matter/grey matter/ventricles.
\item f1-f3: Coordinate features, f4: T1 intensity, f5: T1 gradient, f6: T2 intensity, f7: T2 gradient.
\end{tablenotes}
\end{threeparttable}
\end{table*}


\begin{table*}[t]
\renewcommand{\arraystretch}{1.2}
\newcommand\mulrow[2]{\multirow{#1}{*}{\shortstack[c]{#2}}}
\caption{Runtime}
\label{tab:time_compare}
\centering
\begin{threeparttable}
\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}}c*{6}{S[table-number-alignment=center,table-figures-decimal=2,table-auto-round]}@{}}
\toprule
Features & {Size Dataset} & {\shortstack[c]{DF}} & {\shortstack[c]{GMM}} & {\shortstack[c]{kNN}} & {\shortstack[c]{SGD}} & {\shortstack[c]{SVM}}\\
\midrule
\mulrow{3}{All\\(f1-f7)}
	& 3		&	{-}		& {-}	& {14.4/6564.6}	& {-}	& {-}\\
	& 12		&	{-}		& {-}	& {39.6/7548.7}	& {-}	& {-}\\
	& 70		&	{-}		& {-}	& {223.1/8724.6}	& {-}	& {-}\\
\midrule
\mulrow{3}{Coordinates only\\(f1-f3)}
	& 3		&	{-}		& {-}	& {10.4/4391.5}	& {-}	& {-}\\
	& 12		&	{-}		& {-}	& {34.7/5449.3}	& {-}	& {-}\\
	& 70		&	{-}		& {-}	& {196.4/6112.8}	& {-}	& {-}\\
\midrule
\mulrow{3}{All non-coordinates \\(f4-f7)}
	& 3		&	{-}		& {-}	& {10.1/10084.7}	& {-}	& {-}\\
	& 12		&	{-}		& {-}	& {34.6/18768.6}	& {-}	& {-}\\
	& 70		&	{-}		& {-}	& {194.2/16555.7}	& {-}	& {-}\\
\bottomrule
\end{tabular*}
\begin{tablenotes}
\item \FIXME{Overview of the computation time in seconds for all algorithms (training time/testing time). Computation time includes pre- and post-processing.}
\end{tablenotes}
\end{threeparttable}
\end{table*}a


\section{Discussion}
\TODO{challenge with quality of ground truth}

\TODO{feature importance}


\section{Conclusion}

\section*{Acknowledgment}
Calculations were performed on UBELIX (http://www.id.unibe.ch/hpc), the HPC cluster at the University of Bern.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}